{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\ndata = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ndata = np.array(data)\nm, n = data.shape\nnp.random.shuffle(data)\n\ndata_dev = data[0:1000].T\nY_dev = data_dev[0]\nX_dev = data_dev[1:n].T\nX_dev = X_dev / 255.0\n\ndata_train = data[1000:m].T\ny_train = data_train[0]\nx_train = data_train[1:n].T\nx_train = x_train / 255.0\n_, m_train = x_train.shape\n\nclass Linear:\n    \"\"\"\n    A linear (fully connected) layer in a neural network.\n    \n    Parameters:\n    - input_dim (int): Number of input features.\n    - output_dim (int): Number of output features.\n    \"\"\"\n    def __init__(self, input_dim, output_dim):\n        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n        self.bias = np.zeros((1, output_dim))\n\n    def forward(self, X):\n        \"\"\"\n        Performs the forward pass through the linear layer.\n        \n        Parameters:\n        - X (ndarray): Input data of shape (batch_size, input_dim).\n        \n        Returns:\n        - ndarray: Output data of shape (batch_size, output_dim).\n        \"\"\"\n        self.input = X\n        return np.dot(X, self.weights) + self.bias\n\n    def backward(self, grad_output):\n        \"\"\"\n        Performs the backward pass and computes the gradients.\n        \n        Parameters:\n        - grad_output (ndarray): Gradient of the loss with respect to the output.\n        \n        Returns:\n        - ndarray: Gradient of the loss with respect to the input.\n        \"\"\"\n        grad_input = np.dot(grad_output, self.weights.T)\n        self.grad_weights = np.dot(self.input.T, grad_output)\n        self.grad_bias = np.sum(grad_output, axis=0, keepdims=True)\n        return grad_input\n\nclass ReLU:\n    \"\"\"\n    ReLU (Rectified Linear Unit) activation layer.\n    \"\"\"\n    def forward(self, X):\n        \"\"\"\n        Applies the ReLU activation function.\n        \n        Parameters:\n        - X (ndarray): Input data.\n        \n        Returns:\n        - ndarray: Output data after applying ReLU.\n        \"\"\"\n        self.input = X\n        return np.maximum(0, X)\n\n    def backward(self, grad_output):\n        \"\"\"\n        Computes the gradient of the loss with respect to the input.\n        \n        Parameters:\n        - grad_output (ndarray): Gradient of the loss with respect to the output.\n        \n        Returns:\n        - ndarray: Gradient of the loss with respect to the input.\n        \"\"\"\n        grad_input = grad_output * (self.input > 0)\n        return grad_input\n\nclass Softmax:\n    \"\"\"\n    Softmax activation layer.\n    \"\"\"\n    def forward(self, X):\n        \"\"\"\n        Applies the softmax activation function.\n        \n        Parameters:\n        - X (ndarray): Input data.\n        \n        Returns:\n        - ndarray: Output data after applying softmax.\n        \"\"\"\n        exp_values = np.exp(X - np.max(X, axis=1, keepdims=True))\n        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        return self.output\n\n    def backward(self, grad_output):\n        \"\"\"\n        Computes the gradient of the loss with respect to the input.\n        \n        Parameters:\n        - grad_output (ndarray): Gradient of the loss with respect to the output.\n        \n        Returns:\n        - ndarray: Gradient of the loss with respect to the input.\n        \"\"\"\n        return grad_output\n\nclass CrossEntropyLoss:\n    \"\"\"\n    Cross-entropy loss function.\n    \"\"\"\n    def forward(self, y_pred, y_true):\n        \"\"\"\n        Computes the forward pass of the cross-entropy loss.\n        \n        Parameters:\n        - y_pred (ndarray): Predicted probabilities.\n        - y_true (ndarray): True labels.\n        \n        Returns:\n        - float: Loss value.\n        \"\"\"\n        samples = len(y_pred)\n        y_pred_clipped = np.clip(y_pred, 1e-12, 1. - 1e-12)\n        correct_confidences = y_pred_clipped[range(samples), y_true]\n        return -np.mean(np.log(correct_confidences))\n\n    def backward(self, y_pred, y_true):\n        \"\"\"\n        Computes the backward pass of the cross-entropy loss.\n        \n        Parameters:\n        - y_pred (ndarray): Predicted probabilities.\n        - y_true (ndarray): True labels.\n        \n        Returns:\n        - ndarray: Gradient of the loss with respect to the input.\n        \"\"\"\n        samples = len(y_pred)\n        grad = y_pred\n        grad[range(samples), y_true] -= 1\n        return grad / samples\n\nclass SGD:\n    \"\"\"\n    Stochastic Gradient Descent (SGD) optimizer.\n    \n    Parameters:\n    - learning_rate (float): Learning rate for the optimizer.\n    \"\"\"\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n\n    def step(self, layers):\n        \"\"\"\n        Updates the weights and biases of each layer.\n        \n        Parameters:\n        - layers (list): List of layers in the model.\n        \"\"\"\n        for layer in layers:\n            if hasattr(layer, 'weights'):\n                layer.weights -= self.learning_rate * layer.grad_weights\n                layer.bias -= self.learning_rate * layer.grad_bias\n\nclass Model:\n    \"\"\"\n    Neural network model.\n    \"\"\"\n    def __init__(self):\n        self.layers = []\n        self.loss = None\n        self.optimizer = None\n\n    def add_layer(self, layer):\n        \"\"\"\n        Adds a layer to the model.\n        \n        Parameters:\n        - layer: Layer to be added to the model.\n        \"\"\"\n        self.layers.append(layer)\n\n    def compile(self, loss, optimizer):\n        \"\"\"\n        Compiles the model by specifying the loss function and optimizer.\n        \n        Parameters:\n        - loss: Loss function.\n        - optimizer: Optimizer.\n        \"\"\"\n        self.loss = loss\n        self.optimizer = optimizer\n\n    def forward(self, X):\n        \"\"\"\n        Performs the forward pass through all the layers.\n        \n        Parameters:\n        - X (ndarray): Input data.\n        \n        Returns:\n        - ndarray: Output after passing through all layers.\n        \"\"\"\n        for layer in self.layers:\n            X = layer.forward(X)\n        return X\n\n    def backward(self, grad_output):\n        \"\"\"\n        Performs the backward pass through all the layers.\n        \n        Parameters:\n        - grad_output (ndarray): Gradient of the loss with respect to the output.\n        \"\"\"\n        for layer in reversed(self.layers):\n            grad_output = layer.backward(grad_output)\n\n    def train(self, X, y, epochs):\n        \"\"\"\n        Trains the model for a specified number of epochs.\n        \n        Parameters:\n        - X (ndarray): Training data.\n        - y (ndarray): True labels.\n        - epochs (int): Number of epochs to train for.\n        \"\"\"\n        for epoch in range(epochs):\n            y_pred = self.forward(X)\n            loss_value = self.loss.forward(y_pred, y)\n            grad_output = self.loss.backward(y_pred, y)\n            self.backward(grad_output)\n            self.optimizer.step(self.layers)\n            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss_value:.4f}')\n\n    def predict(self, X):\n        \"\"\"\n        Predicts the output for the given input data.\n        \n        Parameters:\n        - X (ndarray): Input data.\n        \n        Returns:\n        - ndarray: Predicted output.\n        \"\"\"\n        return self.forward(X)\n\n    def evaluate(self, X, y):\n        \"\"\"\n        Evaluates the model on the given data.\n        \n        Parameters:\n        - X (ndarray): Input data.\n        - y (ndarray): True labels.\n        \n        Returns:\n        - tuple: Loss value and accuracy.\n        \"\"\"\n        y_pred = self.predict(X)\n        loss_value = self.loss.forward(y_pred, y)\n        accuracy = np.mean(np.argmax(y_pred, axis=1) == y)\n        print(f'Loss: {loss_value:.4f}, Accuracy: {accuracy:.4f}')\n        return loss_value, accuracy\n\nmodel = Model()\nmodel.add_layer(Linear(784, 128))\nmodel.add_layer(ReLU())\nmodel.add_layer(Linear(128, 10))\nmodel.add_layer(Softmax())\n\nloss = CrossEntropyLoss()\noptimizer = SGD(learning_rate=0.2)\nmodel.compile(loss, optimizer)\n\nmodel.train(x_train, y_train, epochs=150)\n\ntest_loss, test_accuracy = model.evaluate(X_dev, Y_dev)\nprint(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}